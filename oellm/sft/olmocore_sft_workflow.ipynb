{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Olmo 3 7B SFT Workflow\n",
        "\n",
        "Complete workflow for training OLMo-3-7B with supervised fine-tuning using OLMo-core.\n",
        "\n",
        "## Step 1: Clone OLMo-core repository\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "REPO_DIR=\"/work/dlclarge2/ferreira-oellm/OLMo-core\"\n",
        "if [ ! -d \"$REPO_DIR\" ]; then\n",
        "  git clone https://github.com/allenai/OLMo-core \"$REPO_DIR\"\n",
        "fi\n",
        "cd \"$REPO_DIR\"\n",
        "git fetch origin\n",
        "git checkout jacobm-final-merge-sft\n",
        "git pull origin jacobm-final-merge-sft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Prepare SFT dataset\n",
        "\n",
        "Convert your SFT data to OLMo-core's tokenized format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!./oellm/sft/prepare_dolci_sft_data.sh /work/dlclarge2/ferreira-oellm/open-instruct/data/dolci_think_sft_tokenized\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Download base model checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python oellm/sft/download_olmo3_7b.py --output-dir models/Olmo-3-1025-7B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Convert HuggingFace checkpoint to OLMo-core format\n",
        "\n",
        "The downloaded HuggingFace checkpoint needs to be converted to OLMo-core's native format (.distcp files) before it can be used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "OLMOCORE_PATH=\"/work/dlclarge2/ferreira-oellm/OLMo-core\"\n",
        "export PYTHONPATH=\"${OLMOCORE_PATH}/src:${PYTHONPATH:-}\"\n",
        "\n",
        "# Convert HuggingFace checkpoint to OLMo-core format\n",
        "python \"${OLMOCORE_PATH}/src/examples/huggingface/convert_checkpoint_from_hf.py\" \\\n",
        "  -i allenai/Olmo-3-1025-7B \\\n",
        "  -m olmo3_7b \\\n",
        "  -o models/Olmo-3-1025-7B-olmocore\n",
        "\n",
        "# Copy metadata to root for checkpoint recognition\n",
        "cp models/Olmo-3-1025-7B-olmocore/model_and_optim/.metadata models/Olmo-3-1025-7B-olmocore/.metadata\n",
        "\n",
        "echo \"Conversion complete! Checkpoint saved to models/Olmo-3-1025-7B-olmocore\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Launch SFT Training\n",
        "\n",
        "Now that the dataset is prepared and the checkpoint is converted, you can launch the training:\n",
        "\n",
        "**Notes:**\n",
        "- The script uses the converted OLMo-core checkpoint at `models/Olmo-3-1025-7B-olmocore`\n",
        "- Logs will be saved to `slurm_logs/olmo3-7b-think-sft/`\n",
        "- Training checkpoints will be saved to `checkpoints/ferreira/olmo3-7b-sft/dolci-think-sft/`\n",
        "- The script is configured for 2 GPUs by default (for testing). For full training, use 8 GPUs on the `alldlc2_gpu-h200` partition\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!sbatch oellm/sft/train_think_sft_dolci_7b_slurm.sh\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
